{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EEGModels import EEGNet\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import balanced_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'P_1.mat', 1: 'P_10.mat', 2: 'P_2.mat', 3: 'P_3.mat', 4: 'P_4.mat', 5: 'P_5.mat', 6: 'P_9.mat', 7: 'S_1.mat', 8: 'S_10.mat', 9: 'S_2.mat', 10: 'S_3.mat', 11: 'S_4.mat', 12: 'S_5.mat', 13: 'S_9.mat'}\n",
      "{0: 'P_1.mat', 1: 'P_10.mat', 2: 'P_2.mat', 3: 'P_3.mat', 4: 'P_4.mat', 5: 'P_5.mat', 6: 'P_9.mat', 7: 'S_1.mat', 8: 'S_10.mat', 9: 'S_2.mat', 10: 'S_3.mat', 11: 'S_4.mat', 12: 'S_5.mat', 13: 'S_9.mat'}\n",
      "(79, 14, 750) (79, 6)\n",
      "(56, 14, 750) (56, 6)\n",
      "(78, 14, 750) (78, 6)\n",
      "(80, 14, 750) (80, 6)\n",
      "(116, 14, 750) (116, 6)\n",
      "(85, 14, 750) (85, 6)\n",
      "(89, 14, 750) (89, 6)\n",
      "(104, 14, 750) (104, 6)\n",
      "(77, 14, 750) (77, 6)\n",
      "(89, 14, 750) (89, 6)\n",
      "(119, 14, 750) (119, 6)\n",
      "(95, 14, 750) (95, 6)\n",
      "(100, 14, 750) (100, 6)\n",
      "(96, 14, 750) (96, 6)\n",
      "(84, 14, 750) (84, 6)\n",
      "(55, 14, 750) (55, 6)\n",
      "(74, 14, 750) (74, 6)\n",
      "(84, 14, 750) (84, 6)\n",
      "(86, 14, 750) (86, 6)\n",
      "(87, 14, 750) (87, 6)\n",
      "(90, 14, 750) (90, 6)\n",
      "(90, 14, 750) (90, 6)\n",
      "(98, 14, 750) (98, 6)\n",
      "(90, 14, 750) (90, 6)\n",
      "(120, 14, 750) (120, 6)\n",
      "(118, 14, 750) (118, 6)\n",
      "(97, 14, 750) (97, 6)\n",
      "(124, 14, 750) (124, 6)\n"
     ]
    }
   ],
   "source": [
    "folder_path_dance = '../condition_data/dance/raw/inst/'\n",
    "folder_path_walk = '../condition_data/walk/raw/inst/'\n",
    "\n",
    "file_list_dance = [os.path.join(folder_path_dance, file) for file in os.listdir(folder_path_dance)]\n",
    "file_list_walk = [os.path.join(folder_path_walk, file) for file in os.listdir(folder_path_walk)]\n",
    "\n",
    "file_list_dance.sort()\n",
    "file_list_walk.sort()\n",
    "\n",
    "# create a dictionary that maps indices to file names\n",
    "file_dict_dance = {index: file_name.split(\"/\")[-1] for index, file_name in enumerate(file_list_dance)}\n",
    "file_dict_walk = {index: file_name.split(\"/\")[-1] for index, file_name in enumerate(file_list_walk)}\n",
    "\n",
    "print(file_dict_dance)\n",
    "print(file_dict_walk)\n",
    "\n",
    "X_dance = []\n",
    "Y_dance = []\n",
    "X_walk = []\n",
    "Y_walk = []\n",
    "\n",
    "for file in file_list_dance:\n",
    "    dataset = scipy.io.loadmat(file)\n",
    "    x = dataset['data']\n",
    "    y = dataset['labels']\n",
    "\n",
    "    y = np_utils.to_categorical(y-1, num_classes=6)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    X_dance.append(x)\n",
    "    Y_dance.append(y)\n",
    "\n",
    "for file in file_list_walk:\n",
    "    dataset = scipy.io.loadmat(file)\n",
    "    x = dataset['data']\n",
    "    y = dataset['labels']\n",
    "\n",
    "    y = np_utils.to_categorical(y-1, num_classes=6)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    X_walk.append(x)\n",
    "    Y_walk.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../condition_results/data_sizes.txt', 'a')\n",
    "f.write('P_and_S_Dance_Inst: ' + str(sum(x.shape[0] for x in X_dance)) + '\\n')\n",
    "f.write('P_and_S_Walk_Inst: ' + str(sum(x.shape[0] for x in X_walk)) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels, chans, samples = 1, 14, 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../condition_results/P_and_S_inst.txt', 'a')\n",
    "f.write('---------------------------------DANCE----------------------------------\\n')\n",
    "f.close()\n",
    "\n",
    "dance_accuracies = []\n",
    "dance_accuracies_p = []\n",
    "dance_accuracies_s = []\n",
    "\n",
    "for index_1 in range(0,7):\n",
    "    for index_2 in range(7,14):\n",
    "\n",
    "        # build training set from allfiles expect the ones with index_1 and index_2\n",
    "        X_train = np.concatenate([X_dance[i] for i in range(len(X_dance)) if i != index_1 and i != index_2])\n",
    "        Y_train = np.concatenate([Y_dance[i] for i in range(len(Y_dance)) if i != index_1 and i != index_2])\n",
    "\n",
    "        # build test set from the files with index_1 and index_2\n",
    "        X_test = [X_dance[i] for i in range(len(X_dance)) if i == index_1 or i == index_2]\n",
    "        Y_test = [Y_dance[i] for i in range(len(Y_dance)) if i == index_1 or i == index_2]\n",
    "\n",
    "        # convert data to NHWC (trials, channels, samples, kernels) format.\n",
    "        X_train = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "\n",
    "        model = EEGNet(nb_classes=6, Chans=chans, Samples=samples, dropoutRate=0.5, \n",
    "                       kernLength=32, F1=8, D=2, F2=16, dropoutType='Dropout')\n",
    "\n",
    "        # compile the model and set the optimizers\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "        # count number of parameters in the model\n",
    "        numParams    = model.count_params() \n",
    "\n",
    "        # set a valid path for the system to record model checkpoints\n",
    "        # checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "        class_weights = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1., 5: 1.}\n",
    "\n",
    "        fitted_model = model.fit(X_train, Y_train, batch_size = 32, epochs = 50, verbose = 2, \n",
    "                                class_weight=class_weights)\n",
    "        \n",
    "        # make prediction on the 2 test files individually\n",
    "        X_test_1 = X_test[0].reshape(X_test[0].shape[0], chans, samples, kernels)\n",
    "        X_test_2 = X_test[1].reshape(X_test[1].shape[0], chans, samples, kernels)\n",
    "\n",
    "        probs_1 = model.predict(X_test_1)\n",
    "        probs_2 = model.predict(X_test_2)\n",
    "        preds_1 = probs_1.argmax(axis = -1)\n",
    "        preds_2 = probs_2.argmax(axis = -1)\n",
    "        acc_1 = balanced_accuracy_score(Y_test[0].argmax(axis=-1), preds_1)\n",
    "        acc_2 = balanced_accuracy_score(Y_test[1].argmax(axis=-1), preds_2)\n",
    "\n",
    "        # make prediction on the 2 test files together\n",
    "        X_test = np.concatenate(X_test)\n",
    "        X_test = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
    "        Y_test = np.concatenate(Y_test)\n",
    "\n",
    "        probs       = model.predict(X_test)\n",
    "        preds       = probs.argmax(axis = -1)\n",
    "        acc         = balanced_accuracy_score(Y_test.argmax(axis=-1), preds)\n",
    "\n",
    "        dance_accuracies_p.append(acc_1)\n",
    "        dance_accuracies_s.append(acc_2)\n",
    "        dance_accuracies.append(acc)\n",
    "\n",
    "        f = open('../condition_results/P_and_S_inst.txt', 'a')\n",
    "        f.write(\"--------SELECTED FILES: '%s' and '%s'--------\\n\" % (file_dict_dance[index_1], file_dict_dance[index_2]))\n",
    "        f.write(\"Classification accuracy for P: %f \\n\" % (acc_1))\n",
    "        f.write(\"Classification accuracy for S: %f \\n\" % (acc_2))\n",
    "        f.write(\"Classification accuracy for P and S: %f \\n\" % (acc))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../condition_results/P_and_S_inst.txt', 'a')\n",
    "f.write('\\n\\n---------------------------------WALK----------------------------------\\n')\n",
    "f.close()\n",
    "\n",
    "walk_accuracies = []\n",
    "walk_accuracies_p = []\n",
    "walk_accuracies_s = []\n",
    "\n",
    "for index_1 in range(0,7):\n",
    "    for index_2 in range(7,14):\n",
    "\n",
    "        # build training set from allfiles expect the ones with index_1 and index_2\n",
    "        X_train = np.concatenate([X_walk[i] for i in range(len(X_walk)) if i != index_1 and i != index_2])\n",
    "        Y_train = np.concatenate([Y_walk[i] for i in range(len(Y_walk)) if i != index_1 and i != index_2])\n",
    "\n",
    "        # build test set from the files with index_1 and index_2\n",
    "        X_test = [X_walk[i] for i in range(len(X_walk)) if i == index_1 or i == index_2]\n",
    "        Y_test = [Y_walk[i] for i in range(len(Y_walk)) if i == index_1 or i == index_2]\n",
    "\n",
    "        # convert data to NHWC (trials, channels, samples, kernels) format.\n",
    "        X_train = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "\n",
    "        model = EEGNet(nb_classes=6, Chans=chans, Samples=samples, dropoutRate=0.5, \n",
    "                       kernLength=32, F1=8, D=2, F2=16, dropoutType='Dropout')\n",
    "\n",
    "        # compile the model and set the optimizers\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "        # count number of parameters in the model\n",
    "        numParams    = model.count_params() \n",
    "\n",
    "        # set a valid path for the system to record model checkpoints\n",
    "        # checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "        class_weights = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1., 5: 1.}\n",
    "\n",
    "        fitted_model = model.fit(X_train, Y_train, batch_size = 32, epochs = 50, verbose = 2, \n",
    "                                class_weight=class_weights)\n",
    "        \n",
    "        # make prediction on the 2 test files individually\n",
    "        X_test_1 = X_test[0].reshape(X_test[0].shape[0], chans, samples, kernels)\n",
    "        X_test_2 = X_test[1].reshape(X_test[1].shape[0], chans, samples, kernels)\n",
    "\n",
    "        probs_1 = model.predict(X_test_1)\n",
    "        probs_2 = model.predict(X_test_2)\n",
    "        preds_1 = probs_1.argmax(axis = -1)\n",
    "        preds_2 = probs_2.argmax(axis = -1)\n",
    "        acc_1 = balanced_accuracy_score(Y_test[0].argmax(axis=-1), preds_1)\n",
    "        acc_2 = balanced_accuracy_score(Y_test[1].argmax(axis=-1), preds_2)\n",
    "\n",
    "        # make prediction on the 2 test files together\n",
    "        X_test = np.concatenate(X_test)\n",
    "        X_test = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
    "        Y_test = np.concatenate(Y_test)\n",
    "\n",
    "        probs       = model.predict(X_test)\n",
    "        preds       = probs.argmax(axis = -1)\n",
    "        acc         = balanced_accuracy_score(Y_test.argmax(axis=-1), preds)\n",
    "\n",
    "        walk_accuracies_p.append(acc_1)\n",
    "        walk_accuracies_s.append(acc_2)\n",
    "        walk_accuracies.append(acc)\n",
    "\n",
    "        f = open('../condition_results/P_and_S_inst.txt', 'a')\n",
    "        f.write(\"--------SELECTED FILES: '%s' and '%s'--------\\n\" % (file_dict_walk[index_1], file_dict_walk[index_2]))\n",
    "        f.write(\"Classification accuracy for P: %f \\n\" % (acc_1))\n",
    "        f.write(\"Classification accuracy for S: %f \\n\" % (acc_2))\n",
    "        f.write(\"Classification accuracy for P and S: %f \\n\" % (acc))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the accuracies\n",
    "p_acc_dance = np.array(dance_accuracies_p)\n",
    "s_acc_dance = np.array(dance_accuracies_s)\n",
    "acc_dance = np.array(dance_accuracies)\n",
    "\n",
    "s_acc_walk = np.array(walk_accuracies_s)\n",
    "p_acc_walk = np.array(walk_accuracies_p)\n",
    "acc_walk = np.array(walk_accuracies)\n",
    "\n",
    "f = open(\"../condition_results/P_and_S_inst.txt\", \"a\")\n",
    "f.write(\"\\n\\nMean overall accuracy dance: %f \\n\" % (np.mean(acc_dance)))\n",
    "f.write(\"Std overall accuracy dance: %f \\n\" % (np.std(acc_dance)))\n",
    "f.write(\"Mean overall accuracy walk: %f \\n\" % (np.mean(acc_walk)))\n",
    "f.write(\"Std overall accuracy walk: %f \\n\" % (np.std(acc_walk)))\n",
    "f.write(\"\\n\\nMean P accuracy dance: %f \\n\" % (np.mean(p_acc_dance)))\n",
    "f.write(\"Std P accuracy dance: %f \\n\" % (np.std(p_acc_dance)))\n",
    "f.write(\"Mean P accuracy walk: %f \\n\" % (np.mean(p_acc_walk)))\n",
    "f.write(\"Std P accuracy walk: %f \\n\" % (np.std(p_acc_walk)))\n",
    "f.write(\"\\n\\nMean S accuracy dance: %f \\n\" % (np.mean(s_acc_dance)))\n",
    "f.write(\"Std S accuracy dance: %f \\n\" % (np.std(s_acc_dance)))\n",
    "f.write(\"Mean S accuracy walk: %f \\n\" % (np.mean(s_acc_walk)))\n",
    "f.write(\"Std S accuracy walk: %f \\n\" % (np.std(s_acc_walk)))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_net_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
